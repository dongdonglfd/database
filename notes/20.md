# 一、什么是分布式数据库（Distributed DBMS）？

## 1️⃣ 一句话定义（先给直觉）

**分布式数据库 = 一个“看起来像一个数据库”的系统，但实际上数据分散在很多台机器上**

对应用程序来说：

* 它只看到 **一个数据库**
* 不用关心数据在哪台机器
* SQL 怎么写，和单机数据库**一模一样**

对数据库内部来说：

* 数据被拆散放在不同节点
* 查询要跨机器执行
* 事务要跨机器协调
* 机器可能会宕机、网络会慢、会丢包

---

## 2️⃣ 为什么非要用分布式数据库？

两个核心原因：

### ✅ 1. 可扩展性（Scalability）

单机：

* CPU 有上限
* 内存有限
* 磁盘有限

分布式：

* 数据太大 → 多加机器
* 请求太多 → 多加机器
* 用户遍布全球 → 多地部署

👉 **“横向扩展”比“纵向扩展”便宜、灵活**

---

### ✅ 2. 容错性（Fault Tolerance）

单机数据库：

* 一台机器挂了 → 整个数据库挂了 ❌

分布式数据库：

* 一台机器挂了 → 其他机器还能顶上
* 数据有副本
* 系统能继续提供服务

👉 **不把所有鸡蛋放在一个篮子里**

---

# 二、并行数据库 vs 分布式数据库（这点非常关键）

很多人会混，这里一定要讲清楚。

---

## 1️⃣ 并行数据库（Parallel DB）

可以理解为：**“一台大机器里的多核多CPU协作”**

特点：

* 节点离得很近（同机房、同机柜）
* 网络快、稳定
* 默认：

  * 不丢包
  * 不乱序
  * 节点几乎不会挂

👉 设计协议时：

* **可以假设通信几乎是免费的**
* 不太考虑节点失败

---

## 2️⃣ 分布式数据库（Distributed DB）

可以理解为：**“很多普通服务器，可能在不同城市”**

特点：

* 节点可能相距很远
* 网络慢、不稳定
* 随时可能：

  * 机器宕机
  * 网络断
  * 消息丢失 / 延迟

👉 设计协议时：

* **通信成本是核心问题**
* **失败是常态，不是例外**

📌 这也是为什么：

> 分布式数据库 ≠ 把并行数据库放到多台机器上

---

# 三、系统架构：资源是“共享”还是“隔离”？

这决定了：

* 数据怎么放
* 节点怎么协调
* 系统好不好扩展

---

## 1️⃣ Shared Everything（单机数据库）

**所有东西都共享**

* CPU：共享
* 内存：共享
* 磁盘：共享

优点：

* 简单
* 好实现事务
* 好实现一致性

缺点：

* 扩展性差
* 单点故障

---

## 2️⃣ Shared Memory（共享内存）

**多 CPU + 共享内存 + 共享磁盘**

* 所有 CPU 能直接访问同一块内存
* 像一台“超大的服务器”

问题：

* 硬件非常贵
* 扩展困难
* 实际上很少用

👉 **学术上存在，工业界基本不用**

---

## 3️⃣ Shared Disk（共享磁盘）

**CPU 和内存分开，但磁盘共享**

* 每个节点：

  * 有自己的 CPU
  * 有自己的内存
* 所有节点：

  * 访问同一个逻辑磁盘（网络存储）

典型场景：

* 云数据库
* Data Lake
* Serverless DB

### 优点

* 存储和计算可以独立扩展
* 不用迁移数据就能加计算节点

### 缺点

* 内存不共享
* 节点要不停同步“谁改了什么数据”
* 协调成本高

---

## 4️⃣ Shared Nothing（最典型的分布式 DB）

**每个节点完全独立**

* CPU：不共享
* 内存：不共享
* 磁盘：不共享
* 只能靠网络通信

这是：

* **NoSQL**
* **NewSQL**
* **大多数分布式数据库** 的主流架构

### 优点

* 性能好
* 扩展性强
* 成本低（普通服务器）

### 缺点

* 数据要“挪来挪去”
* 分布式事务非常复杂
* 一致性难保证

📌 **分布式数据库的难点，80% 都来自 shared nothing**

---

# 四、分布式 DB 的核心设计问题（非常重要）

系统必须回答这些问题：

---

## 1️⃣ 应用怎么“找到数据”？

* 数据在哪个节点？
* 是自动路由还是应用自己知道？

👉 通常：

* DBMS 内部有 **元数据**
* 应用 **完全无感知**

---

## 2️⃣ 查询发给谁？

* 一个 SQL 查询：

  * 发给一个节点？
  * 还是拆成多个子查询？

---

## 3️⃣ 查询在哪执行？

两种思路：

### 思路 A：**把查询推到数据那里**

* 每个节点只处理自己那部分数据
* 最后汇总结果

👉 常见、推荐

### 思路 B：**把数据拉到一个地方**

* 网络代价巨大
* 一般只在数据很小的时候用

---

## 4️⃣ 数据怎么拆？

👉 这就进入 **分区（Partitioning / Sharding）**

---

## 5️⃣ 正确性怎么保证？

* 多个节点并发事务
* 网络延迟
* 节点宕机

👉 需要：

* 分布式并发控制
* 分布式提交协议（后续会讲 2PC）

---

# 五、数据分区（Partitioning / Sharding）

这是分布式数据库的**灵魂设计**

---

## 核心目标（一定要记住）

> **尽量让一个事务只访问一个分区**

为什么？

* 不用跨节点
* 不用分布式事务
* 性能高、逻辑简单

---

## 1️⃣ 最 naive 的分区（不要这样）

👉 **按表分**

* 表 A → 节点 1
* 表 B → 节点 2

问题：

* JOIN 很痛苦
* 热点严重
* 扩展性极差

---

## 2️⃣ 垂直分区（按列拆）

把一张表的列拆开：

```
User(id, name)     → 节点 1
User(id, address)  → 节点 2
```

问题：

* 查询一条完整记录要跨节点
* 重建 tuple 成本高

👉 **很少作为主方案**

---

## 3️⃣ 水平分区（主流方案）

👉 **按行拆**

```
User 表：
id 1~1M     → 节点 1
id 1M~2M    → 节点 2
```

每个节点存一部分行

---

### 分区键（Partition Key）

这是关键：

* 用哪个列来决定数据去哪？
* 目标：

  * 数据均匀
  * 访问负载均匀
  * 常用查询不跨节点

---

## 4️⃣ 物理分区 vs 逻辑分区

### 物理分区（shared nothing）

* 节点 **真的存数据**
* 典型 NoSQL / NewSQL

### 逻辑分区（shared disk）

* 节点“负责”一部分 key
* 实际数据在共享存储

---

# 六、集群规模变化的问题（非常现实）

问题：

> 如果我加一台机器，数据怎么办？

---

## 1️⃣ 普通 Hash 分区的问题

```
hash(key) % N
```

* N 变了
* **几乎所有数据都要重算**
* 大规模数据迁移 → 灾难

---

## 2️⃣ 一致性哈希（Consistent Hashing）

### 直觉理解

* 把整个 hash 空间想成一个圆
* 节点放在圆上
* key 落到圆上的某个点
* 顺时针第一个节点负责它

### 好处

* 加/减节点：

  * 只影响 **相邻的一小部分数据**
* 平均只移动 **1/N 的数据**

👉 非常适合分布式系统

---

## 3️⃣ Rendezvous Hashing（最高权重哈希）

思路更直接：

* 对一个 key：

  * 对每个节点算一个 hash 分数
  * 谁分数最高，key 就归谁

优点：

* 不需要 hash ring
* 节点变动时：

  * 只有“赢家变了”的 key 才迁移

👉 一致性哈希可以看作它的特例优化

---

# 七、分布式并发控制（谁来协调事务？）

一旦事务跨节点，就必须协调。

---

## 1️⃣ 集中式协调器（Centralized Coordinator）

就像：

> **交通警察**

流程：

1. 客户端 → 协调器
2. 协调器：

   * 管锁
   * 管事务
3. 协调器和各分区通信
4. 决定 commit / abort

### 优点

* 逻辑清晰
* 好实现

### 缺点

* 单点瓶颈
* 单点故障

---

## 2️⃣ 去中心化协调（Decentralized）

流程：

* 客户端随便连一个节点
* 这个节点当 leader
* leader 和其他节点协作
* 最终返回结果

### 优点

* 没有单点
* 扩展性好

### 缺点

* 协议复杂
* 实现难度高

👉 工业界越来越偏向去中心化

---

# 八、联邦数据库（Federated DB）

这是“数据库的数据库”。

---

## 直觉理解

* 多个 **不同 DBMS**

  * MySQL
  * PostgreSQL
  * Oracle
* 用一个中间层“粘起来”
* 对外看成一个数据库

---

## 问题非常多

* 数据模型不同
* SQL 方言不同
* 优化器无法全局优化
* 大量数据复制

👉 常见于：

* 大公司
* 历史系统整合

---

# 最终总结（一句话版）

> **分布式数据库 = 在不可靠网络和可能失败的节点上，假装自己是一个“单机数据库”**

为此它必须解决：

* 数据怎么拆
* 查询怎么跑
* 事务怎么一致
* 节点怎么扩展
* 故障怎么恢复


