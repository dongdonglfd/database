# 一、存储
在存储层级结构的顶端，是距离 CPU 最近的存储设备。这类存储设备速度最快，但体积也最小，价格也最高。距离 CPU 越远，存储设备的体积越大，但速度越慢。同时，这些设备的每 GB 价格也越低。

在层级结构的中间还有一条分界线，将易失性器件与非易失性器件分隔开来。
```c
CPU Registers
↓
CPU Cache (L1/L2/L3)
↓
Memory (DRAM)       ← volatile
-------------------------------
SSD / HDD           ← non-volatile
```
**Volatile Devices（易失性存储）**
定义

断电就丢数据

DBMS 中统一称为：Memory

特性（非常关键）
字节寻址（Byte-addressable）
可以直接访问任意地址

随机访问非常快
**Non-Volatile Devices（非易失性存储）**
定义
断电不丢数据
DBMS 中统一称为：Disk

关键特性
 不能字节寻址
只能 按页（Block / Page）读写

通常：4KB

想读某个字节：
先把 整个 4KB 页读进内存
再访问该字节
==DBMS 的核心目标之一==：
>尽量把随机访问转化为顺序访问

# 二、Access Time & Access Pattern（延迟的数量级）

含义：
磁盘 I/O 是 DBMS 中最贵的操作，没有之一

两种访问模式
1. Random Access（随机访问）
每次跳到不连续的位置
2. Sequential Access（顺序访问）
连续读
磁盘 / SSD 都快

**DBMS 的策略**

能顺序就顺序

随机写 → 先写到内存 buffer，再后台刷盘
# 三、System Design Goals（系统设计目标）
目标一句话总结：
让 DBMS 能管理“比内存大得多”的数据库，同时尽量减少磁盘 I/O 的等待

**核心挑战**

数据在 disk

计算在 memory

来回搬数据很慢

**解决方案思想**

自己管理内存

精细控制什么时候读 / 写磁盘
# 四、Disk-Oriented DBMS 总体架构
**Database on Disk**

数据存放在 文件（files） 中

文件内部被切分成 Pages

**Buffer Pool（核心组件）**
Buffer Pool 是什么？

内存中的一块区域，用来缓存磁盘页

工作流程

1. 执行引擎需要 page X

2. 向 buffer pool 请求

3. 如果在内存：直接返回指针

4. 如果不在：
从 disk 读 page
放进 buffer pool
再返回
 **Buffer Pool Manager 的职责**
页的加载 / 驱逐
确保页在使用期间不被踢掉
# 五、File Storage（DBMS 如何使用文件）
**数据库存储形式**

一个文件（SQLite）

多个文件（表 / 索引分开）

文件格式

私有格式（传统）

开放格式（新趋势）

OS 完全不知道文件内部结构

**Storage Manager（存储管理器）**

职责：

把文件看成 一组 pages

跟踪：

哪些页被用

哪些页空闲

页类型

页的脏状态
# 六、Database Pages（数据库页）
**Page 是什么？**

DBMS 管理磁盘的最小单位
特点
固定大小
通常 1KB–16KB

 **Page ID**
Page ID ≠ 文件偏移（逻辑 vs 物理）



好处：

迁移文件不影响上层

支持多文件
# 七、Database Heap（堆文件）
**Heap File Organization**
无序
插入快
查询慢（全表扫描）
适合：
OLTP 插入
没索引的表
 **Page Directory（页目录）**
作用：
Page ID → 物理位置
记录：
页类型
空闲空间
是否可用
# 八、Page Layout（页内部结构）
**Page Header（页头）**

包含：

Page size

Checksum

DBMS version

事务可见性信息

自包含信息

Tuple-Oriented Storage（重点）
Slotted Page（槽页结构）
```c
[Header | Slot Array → ← Tuple Data]
```
==工作方式==

Slot Array 从前往后长

Tuple Data 从后往前长

中间相遇 → 页满

优点

支持删除

支持变长字段

slot 不变 → tuple 可移动

 几乎所有 row-store DBMS 都用它
 # 九、为什么 DBMS 的存储这么复杂？
 一句话先给结论：

>数据库不是“存数据”，而是在“用最少的 I/O，把数据又快又稳地拿出来”

磁盘（SSD / HDD）有三个致命特点：

慢（比内存慢几个数量级）

一次读一整块（page）

随机读写很贵，顺序读写便宜

所以数据库系统的所有设计，几乎都是在和这三点斗争。
# 十、Buffer Pool 优化（内存缓冲池）
**Buffer Pool 是干嘛的？**

可以把它理解为：

数据库自己的“内存缓存”

👉 数据在磁盘
👉 想用？必须先拷到内存
👉 Buffer Pool 就是数据库自己管理的一块内存

⚠️ 关键点：
数据库不信任操作系统的缓存
👉 因为 DBMS 比 OS 更懂“接下来要用什么数据”

 **Multiple Buffer Pools（多个缓冲池）**
为什么要多个？

打个比方：

你有一个仓库：

冷冻食品

易碎品

高频取用的日用品

你会全部混在一个仓库里吗？
当然不会。

数据库也是一样。

DBMS 可以按不同维度拆 Buffer Pool：

每个数据库一个

每张表一个

每种页面类型一个

数据页

索引页

日志页

💡 好处

减少锁竞争（latch contention）

提高局部性

不同数据用不同淘汰策略

 **怎么决定“这个 page 放哪个 buffer pool”？**
方法一：Object ID（对象 ID）

👉 在 Record ID 里加一个“对象编号”

[ObjectID | PageID | SlotID]


ObjectID → 表 / 索引 / 类型

DBMS 维护 ObjectID → Buffer Pool 的映射

✅ 精细控制
❌ 多占空间，管理复杂

方法二：Hash（哈希）
buffer_pool_id = hash(page_id) % N

简单
均匀

工业界常用

4️⃣ Pre-fetching（预取）
 生活类比

你在看书：

看第 10 页

同时把第 11、12 页翻开

因为你知道接下来肯定要用

 DBMS 怎么做？

执行计划已经知道：

要顺序扫表

要扫 B+ 树叶子节点

👉 当前页在处理
👉 后面的页提前读进 Buffer Pool



5️⃣ Scan Sharing（同步扫描）
场景
两个查询：
```c
SELECT * FROM big_table;
SELECT COUNT(*) FROM big_table;
```

如果分开扫：

磁盘 I/O 翻倍

Scan Sharing 做什么？

第一个查询启动扫描

第二个查询“搭便车”

共用一个扫描游标

DBMS 记录：

第二个查询从哪里加入

扫到末尾后再补扫前面的

✅ 正确
✅ 高效

⚠️ Continuous Scan Sharing（学术玩具）

一直在后台扫表
👉 查询随时“跳上来”

❌ 云存储按 I/O 计费 → 非常贵

⭐ Buffer Pool 核心思想总结

DBMS 比 OS 更聪明，因为它知道：

查询计划

数据访问模式

所以 DBMS 能更好地：
决定 淘汰谁
决定 提前读谁
决定 给谁多点内存
# 十一、Tuple-Oriented Storage（面向元组存储）
**基本结构（Slotted Page）**

每个 Page：
```sql
+---------------------+
| Page Header         |
| Slot Array          |  <- slot -> tuple offset
|                     |
| Free Space          |
|                     |
| Tuple Data          |
+---------------------+
```
 **读取一个 tuple 的完整流程**

用 Record ID（RID）：

RID = (page_id, slot_id)


步骤：

查 Page Directory → 找到 page 在磁盘哪

把 page 读进 Buffer Pool

用 slot_id → 找到 tuple 偏移

读取 tuple

** 插入 tuple**

步骤：

找有空 slot 的 page

把 page 读进内存

看剩余空间够不够

不够 → 新建 page

写 tuple + 更新 slot array

 **更新 tuple（问题开始了）**
情况一：新数据 能放下

👉 原地更新（OK）

情况二：新数据 放不下

👉 标记旧 tuple 删除
👉 插入新 tuple（新位置）

5️⃣ **面向元组存储的问题**
 1. 碎片化
删除留下洞
Page 用不满

2. 无意义的磁盘 I/O

只改 1 条 tuple

但必须读 / 写 整个 page

3. 随机 I/O

更新 20 条记录

可能跳 20 个 page

磁盘最怕这个

 那如果系统不允许原地更新呢？
👉 这就引出了 Log-Structured Storage

# 十二、、Log-Structured Storage（日志结构存储）

核心思想一句话：

>不改旧数据，只写新数据

 **基本组成**
 MemTable（内存）
存最近的修改
PUT / DELETE
快

SSTable（磁盘）
MemTable 满了
排序后一次性写盘
不可修改（immutable）
顺序写 → 快

**写入流程**
修改进 MemTable
顺序写日志
MemTable 满 → flush 成 SSTable
每条记录包含：
Key（tuple ID）
操作类型（PUT / DELETE）
内容（如果是 PUT）

 **读取流程（复杂）**

顺序：
查 MemTable
查最新 SSTable
查更老的 SSTable
找最新的 PUT / DELETE

❌ 直接扫会很慢
 **Compaction（压缩合并）**
 类比
你记账：
一条收入
一条支出
一条更正
时间久了账本很乱
👉 重新整理，只留最终结果

**Compaction 做什么？**

合并多个 SSTable
对同一个 key，只保留最新版本
减少文件数量
加快读取
两种主流策略
1. Universal Compaction
所有 SSTable 在一个大层
触发条件：数量 / 重叠过多

2. Level Compaction（LSM Tree）
Level 0 → Level 1 → Level 2
越往下越大
同一层 key 不重叠（L0 例外）

 **Log-Structured Storage 的权衡**

 顺序写，非常快
 读慢
 Compaction 贵
 写放大（写一次 → 多次磁盘写）
 # 十三、Index-Organized Storage（索引组织存储）

前面几种方式：

👉 表是无序的
👉 必须靠索引找 tuple

**Index-Organized 的核心思想**
表本身，就是索引
比如：
B+ Tree
Skip List
Trie
叶子节点里 直接存 tuple
**特点**
天然有序
范围查询非常快
不需要额外索引定位

**代价**
插入 / 更新成本高
结构维护复杂

